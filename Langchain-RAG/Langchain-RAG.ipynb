{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac377704-5c10-459b-8aca-59896fd339a4",
   "metadata": {},
   "source": [
    "RAG - Retrieval Augmented Generation\n",
    "RAG is the technique that enhances language models by combining them with a retrieval system. It allows the model to access and utilize external knowledge when generating responses.\n",
    "\n",
    "-Load\n",
    "-Split\n",
    "-Embed\n",
    "-Store (Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "805e1440-1705-4887-911e-c88be541a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.19\n",
      "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\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "print(langchain.__version__)\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "openai_api_key=os.getenv(\"openai_api_key\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found.\")\n",
    "\n",
    "langchain_api_key=os.getenv(\"langchain_api_key\")\n",
    "if not langchain_api_key:\n",
    "    raise ValueError(\"LANGCHAIN_API_KEY not found.\")\n",
    "\n",
    "print(openai_api_key)\n",
    "print(langchain_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3df5ec6-541c-4823-882e-db21bcd3d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=openai_api_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=langchain_api_key\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"Langchain-RAG\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac0dbc8b-e145-4e9a-982d-3d51f490435a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Good morning! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 11, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_13eed4fce1', 'finish_reason': 'stop', 'logprobs': None}, id='run-fbb068a8-42ff-47d1-9748-f6b9a2896e30-0', usage_metadata={'input_tokens': 11, 'output_tokens': 11, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_response=llm.invoke(\"Hi, good morning\")\n",
    "\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cd363dd-600f-43f9-aacc-93be2f8a9cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good morning! How can I assist you today?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parsers=StrOutputParser()\n",
    "output_parsers.invoke(llm_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07385f65-861e-4ea9-b773-c4cf56fa30c8",
   "metadata": {},
   "source": [
    "Structure Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce02c69a-d42b-4f59-a6ae-49b48044eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class MobileReview(BaseModel):\n",
    "    phone_model:str=Field(description=\"Name and model of the phone\")\n",
    "    rating: float=Field(description=\"Overall rating out of 5\")\n",
    "    pros:List[str]=Field(description=\"List of positive aspects\")\n",
    "    cons:List[str]=Field(description=\"List of negative aspects\")\n",
    "    summary:str=Field(description=\"Brief summary of the review\")\n",
    "\n",
    "review_text=\"\"\"\n",
    "iPhone 16 Review (4.5/5)\n",
    "The iPhone 16 refines Apple’s formula with better performance, improved battery life, and subtle design tweaks. While not a game-changer, it’s a solid upgrade for those with older models.\n",
    "\n",
    "Pros:\n",
    "A18 Bionic chip delivers top-tier performance\n",
    "Longer battery life with optimized efficiency\n",
    "Brighter and smoother display with ProMotion 2.0\n",
    "Camera enhancements for low-light photography\n",
    "\n",
    "Cons:\n",
    "Minimal design changes from the iPhone 15\n",
    "Still no USB-C fast charging improvements\n",
    "High price, especially for base storage\n",
    "\n",
    "Verdict: If you're coming from an iPhone 14 or older, it's a great upgrade. But if you have an iPhone 15, you might want to wait for next year.\n",
    "\"\"\"\n",
    "\n",
    "structured_llm = llm.with_structured_output(MobileReview)\n",
    "output=structured_llm.invoke(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70cb502c-fa5a-4d5d-8778-d3c51a7cedc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileReview(phone_model='iPhone 16', rating=4.5, pros=['A18 Bionic chip delivers top-tier performance', 'Longer battery life with optimized efficiency', 'Brighter and smoother display with ProMotion 2.0', 'Camera enhancements for low-light photography'], cons=['Minimal design changes from the iPhone 15', 'Still no USB-C fast charging improvements', 'High price, especially for base storage'], summary='The iPhone 16 refines Apple’s formula with better performance, improved battery life, and subtle design tweaks. While not a game-changer, it’s a solid upgrade for those with older models.')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfae92-c33c-4bf2-aa38-60c9d55be312",
   "metadata": {},
   "source": [
    "LLM Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c106127-afa8-4103-98c7-9cf58bfa29a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to facilitate the development of applications that utilize large language models (LLMs). It provides tools and structures for integrating LLMs into various applications, enabling functionalities such as more efficient data handling, interaction with external systems, and improved model performance.\n",
      "\n",
      "Here are some key features of LangChain:\n",
      "\n",
      "1. **Modularity**: LangChain is built in a modular way, allowing developers to utilize different components depending on their needs. This includes tools for prompt management, memory, and chains of calls to various models.\n",
      "\n",
      "2. **Chains**: One of the core concepts in LangChain is the “chain.” A chain allows you to sequence multiple calls to language models or other functions, enabling more complex workflows. For instance, you can have a chain that first processes some data, then generates a response using an LLM, and finally formats or stores that response.\n",
      "\n",
      "3. **Memory**: LangChain supports concepts of memory, allowing applications to remember previous interactions and context. This can enhance conversational applications by providing continuity and context-awareness.\n",
      "\n",
      "4. **Integrations**: LangChain can integrate with various external data sources and services, making it easier to pull in information from databases, APIs, and other systems to enrich the outputs generated by the language model.\n",
      "\n",
      "5. **Prompt Management**: Effective prompt crafting is crucial for using LLMs effectively. LangChain provides tools to manage and optimize prompts, including template management and prompts that adapt based on user input or other factors.\n",
      "\n",
      "6. **Testing and Evaluation**: LangChain includes tools for testing and benchmarking LLM performances, enabling developers to validate the effectiveness of their applications and make improvements based on quantitative metrics.\n",
      "\n",
      "7. **Use Cases**: LangChain is suitable for a wide range of applications, including chatbots, automated content generation, data analysis, and any scenario where human-like text production or interaction is desirable.\n",
      "\n",
      "Overall, LangChain streamlines working with large language models, enhancing their practical applications in real-world scenarios, and enabling developers to build robust tools that leverage the power of LLMs efficiently.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"You are a smart engineer.\")\n",
    "human_message = HumanMessage(content=\"Tell me about LLM in Langchain\")\n",
    "output_parser=StrOutputParser()\n",
    "chain= llm | output_parser\n",
    "result=chain.invoke([system_message,human_message])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df5666-f3cf-4344-b68a-3182d79541af",
   "metadata": {},
   "source": [
    "--RAG--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2efb038d-6b57-46c9-9b58-5183a0ead9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "#txt_loader=TextLoader(\"docs/DanielTEC.txt\")\n",
    "#documents=txt_loader.load()\n",
    "#txt_loader=TextLoader(\"docs/GreenCompany.txt\")\n",
    "#documents=txt_loader.load()\n",
    "#docx_loader=Docx2txtLoader(\"docs/LuxuryYachtsDV.docx\")\n",
    "#documents=docx_loader.load()\n",
    "#pdf_loader=PyPDFLoader(\"docs/DanielBank.pdf\")\n",
    "#documents=pdf_loader.load()\n",
    "\n",
    "#splits=text_splitter.split_documents(documents)\n",
    "\n",
    "#print(f\"Split the document into {len(splits)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e5950c9-12a9-4655-9512-5c55393a8edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 documents from the folder.\n",
      "Split the documents into 13 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Function to load documents\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif filename.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        elif filename.endswith('.txt'):\n",
    "            loader = TextLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {filename}\")\n",
    "            continue\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "# Load documents from a folder\n",
    "folder_path = \"docs\"\n",
    "documents = load_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} documents from the folder.\")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split the documents into {len(splits)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d21a915a-42d7-4ebf-ba66-268f8416a2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 13 document chunks.\n"
     ]
    }
   ],
   "source": [
    "embeddings=OpenAIEmbeddings()\n",
    "document_embeddings=embeddings.embed_documents([split.page_content for split in splits])\n",
    "\n",
    "print(f\"Created embeddings for {len(document_embeddings)} document chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b4436-b165-479b-8a82-74ed2061420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5719c351-e528-44ae-9a1b-ca6c56b53f4c",
   "metadata": {},
   "source": [
    "Create and persist Chroma vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84ae71-cb92-4a4f-b54c-ae006adb5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchaing_chroma import Chroma\n",
    "\n",
    "embedding_function=OpenAIEmbeddings()\n",
    "collection_name=\"first_RAG_collection\"\n",
    "vector_store=Chroma.from_documents(collections_name=collection_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
